{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "### CHOOSE GPU ###\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "### SETUP CONSTANTS ###\n",
    "\n",
    "import torch\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "RESPONSE_REGEX = r\"### Response:\\s*(.*?)(?=\\n###|\\Z)\"\n",
    "\n",
    "BASE = False    # Load the base model or fine-tuned model\n",
    "MODE = \"BASE\" if BASE else \"FT\" \n",
    "\n",
    "MODEL_LABEL = \"qwen-7B\"     # used to create results file name\n",
    "ITERATION = 20      # iteration to target for fine-tuned model\n",
    "CHECKPOINT_DIR = \"checkpoint-522\"\n",
    "\n",
    "BASE_MODEL_PATH = \"unsloth/Qwen2.5-Coder-7B\"    # used to load the base model\n",
    "FINE_TUNED_MODEL_PATH = f\"./{MODEL_LABEL}/{MODEL_LABEL}-{ITERATION}/checkpoints/{CHECKPOINT_DIR}\"       # can also sub with any path to a fine-tuned model\n",
    "EMBEDDING_MODEL_PATH = \"nvidia/NV-Embed-v2\"\n",
    "\n",
    "METRICS_DIR = \"./summary_stats/metrics\"\n",
    "\n",
    "LOAD_IN_4BIT = False\n",
    "\n",
    "# LoRA Parameters\n",
    "LORA_ALPHA = 64\n",
    "LORA_R = 32\n",
    "LORA_DROPOUT = 0\n",
    "\n",
    "# File paths\n",
    "TRAIN_FILE = \"../train.json\"\n",
    "VALID_FILE = \"../valid.json\"\n",
    "TEST_FILE = \"../test.json\"\n",
    "\n",
    "# Training Hyperparameters\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WARMUP_STEPS = 100      # 5-10% of total steps was suggested\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "MAX_STEPS = 0\n",
    "LEARNING_RATE = 1e-4\n",
    "FP16 = not is_bfloat16_supported()\n",
    "BF16 = is_bfloat16_supported()\n",
    "LOGGING_STEPS = 10\n",
    "OPTIM = \"adamw_8bit\"\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR_SCHEDULER_TYPE = \"linear\"\n",
    "SEED = 3407\n",
    "REPORT_TO = \"none\"  # Use this for WandB etc\n",
    "EVAL_STEPS = 10\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ALPACA_PROMPT = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.8: Fast Qwen2 patching. Transformers: 4.46.2.\n",
      "   \\\\   /|    NVIDIA RTX 6000 Ada Generation. Num GPUs = 1. Max memory: 47.467 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8be5be377554c28b1767d34ac4dc5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(152064, 3584, padding_idx=151665)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=18944, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=18944, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### LOAD THE BASE MODEL ###\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_name = BASE_MODEL_PATH if BASE else FINE_TUNED_MODEL_PATH\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = LOAD_IN_4BIT # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_R, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    lora_dropout = LORA_DROPOUT, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = load_in_4bit,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a header comment for this x86 assembly c...</td>\n",
       "      <td>\\tSECTION .text\\n\\tglobal main\\nmain:\\n\\tpush\\...</td>\n",
       "      <td>; This is the funniest snippet so far. It illu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a header comment for this x86 assembly c...</td>\n",
       "      <td>section .text\\n\\n    global _start\\n\\n_start:\\...</td>\n",
       "      <td>/*\\n * $Id: gets-linux.c,v 1.3 2004/06/02 12:2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write a header comment for this x86 assembly c...</td>\n",
       "      <td>_start:\\n    push   0x66\\n    pop    eax\\n    ...</td>\n",
       "      <td>/*\\n*  Title:    Shell Reverse TCP Shellcode -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Write a header comment for this x86 assembly c...</td>\n",
       "      <td>\\t\\n\\t\\n\\tSECTION .text\\n\\tglobal main\\nmain:\\...</td>\n",
       "      <td>; This snippet copies rcx bytes of the 3-bytes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write a header comment for this x86 assembly c...</td>\n",
       "      <td>\\tSECTION .text\\n\\tglobal main\\nmain:\\n\\t\\n\\tn...</td>\n",
       "      <td>; This snippet sets rax to 1 for all its initi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Write a header comment for this x86 assembly c...   \n",
       "1  Write a header comment for this x86 assembly c...   \n",
       "2  Write a header comment for this x86 assembly c...   \n",
       "3  Write a header comment for this x86 assembly c...   \n",
       "4  Write a header comment for this x86 assembly c...   \n",
       "\n",
       "                                               input  \\\n",
       "0  \\tSECTION .text\\n\\tglobal main\\nmain:\\n\\tpush\\...   \n",
       "1  section .text\\n\\n    global _start\\n\\n_start:\\...   \n",
       "2  _start:\\n    push   0x66\\n    pop    eax\\n    ...   \n",
       "3  \\t\\n\\t\\n\\tSECTION .text\\n\\tglobal main\\nmain:\\...   \n",
       "4  \\tSECTION .text\\n\\tglobal main\\nmain:\\n\\t\\n\\tn...   \n",
       "\n",
       "                                              output  \n",
       "0  ; This is the funniest snippet so far. It illu...  \n",
       "1  /*\\n * $Id: gets-linux.c,v 1.3 2004/06/02 12:2...  \n",
       "2  /*\\n*  Title:    Shell Reverse TCP Shellcode -...  \n",
       "3  ; This snippet copies rcx bytes of the 3-bytes...  \n",
       "4  ; This snippet sets rax to 1 for all its initi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### read the Alpaca JSON file ###\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Function to load JSON file into a Pandas DataFrame\n",
    "def load_json_to_df(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # Assumes JSON contains a list of dictionary entries\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load datasets as DataFrames\n",
    "test_df = load_json_to_df(TEST_FILE)\n",
    "\n",
    "# Convert DataFrames into Hugging Face datasets\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Display first few rows of Train DataFrame\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23d2ec1f0d5411f9dfc2883e49bf99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Load the alpaca format dataset into the correct format ###\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = ALPACA_PROMPT.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "test_dataset = test_dataset.map(formatting_prompts_func, batched = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINER FOR FT MODEL ###\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "def generate_trainer(model, tokenizer, train_dataset, valid_dataset):\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = train_dataset,  # train dataset\n",
    "        eval_dataset=valid_dataset,     # Validation data\n",
    "        dataset_text_field = \"text\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        dataset_num_proc = 2,\n",
    "        packing = False,\n",
    "        args = TrainingArguments(\n",
    "            per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            warmup_steps=WARMUP_STEPS,\n",
    "            num_train_epochs=NUM_TRAIN_EPOCHS, \n",
    "            max_steps=MAX_STEPS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            fp16=FP16,\n",
    "            bf16=BF16,\n",
    "            logging_steps=LOGGING_STEPS,\n",
    "            evaluation_strategy=\"steps\",  \n",
    "            eval_steps=EVAL_STEPS,\n",
    "            save_strategy=\"epoch\",\n",
    "            optim=OPTIM,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "            seed=SEED,\n",
    "            output_dir=METRICS_DIR,\n",
    "            report_to=REPORT_TO,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CALCULATE CROSS ENTROPY LOSS ###\n",
    "\n",
    "def calculate_cross_entropy_loss(trainer, tokenizer, test_dataset):\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=2048, truncation=True)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, batch_size=16)\n",
    "\n",
    "    test_results = trainer.evaluate(tokenized_test_dataset)\n",
    "\n",
    "    cross_entropy_loss = test_results['eval_loss']\n",
    "\n",
    "    print(f\"Test Loss: {test_results['eval_loss']:.16f}\")\n",
    "    return cross_entropy_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = generate_trainer(model, tokenizer, test_dataset, test_dataset)\n",
    "\n",
    "# cross_entropy_loss = og_calculate_cross_entropy_loss(tokenizer, trainer, test_dataset)\n",
    "cross_entropy_loss = calculate_cross_entropy_loss(trainer, tokenizer, test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Embedding Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS FOR CALCULATING COSINE SIMILARITY ### \n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to move model to CUDA\n",
    "def move_model(model, device):\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "# Function to move model to CPU (offload from GPU)\n",
    "def offload_model(model):\n",
    "    return model.to(\"cpu\")\n",
    "\n",
    "\n",
    "# Function to extract only the response part of the model output\n",
    "def extract_response(text):\n",
    "    match = re.search(RESPONSE_REGEX, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "\n",
    "# Function to generate responses and then offload the model\n",
    "def generate_responses(model, tokenizer, test_dataset, device, batch_size=8, max_length=512):\n",
    "    \"\"\"Generates responses, then moves model off CUDA to free memory.\"\"\"\n",
    "    \n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "    # Move model to CUDA\n",
    "    model = move_model(model, device)\n",
    "    FastLanguageModel.for_inference(model)  # Enable optimized inference\n",
    "    \n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "    test_texts = [\n",
    "        ALPACA_PROMPT.format(sample[\"instruction\"], sample[\"input\"], \"\")\n",
    "        for sample in test_dataset\n",
    "    ]\n",
    "\n",
    "    dataloader = DataLoader(test_texts, batch_size=batch_size, shuffle=False)\n",
    "    generated_responses = []\n",
    "\n",
    "    print(\"============= GENERATING RESPONSES =============\")\n",
    "    count = 0\n",
    "\n",
    "    for batch_inputs in dataloader:\n",
    "        print(\"Num Completed: \" + str(count))\n",
    "\n",
    "        input_ids = tokenizer(batch_inputs, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(\"cuda\")\n",
    "        # input_ids = {k: v.to(device) for k, v in input_ids.items()}  # Move to GPU\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # try settign either of these to false: include_prompt_in_result or return_full_text\n",
    "            output_ids = model.generate(input_ids[\"input_ids\"], \n",
    "                                        max_new_tokens=max_length, \n",
    "                                        use_cache=True, \n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        attention_mask=input_ids[\"attention_mask\"]\n",
    "                                    )\n",
    "            batch_outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            for output in batch_outputs:\n",
    "                print(output)\n",
    "                response_text = extract_response(output)\n",
    "                print(response_text)\n",
    "                generated_responses.append(response_text)\n",
    "                print(\"---------------------------------------------\")\n",
    "\n",
    "        count += batch_size\n",
    "        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "    return generated_responses\n",
    "\n",
    "\n",
    "# Function to get batch embeddings\n",
    "def get_batch_embeddings(texts, tokenizer, model, device, max_length=512):\n",
    "    \"\"\"Encodes a batch of texts into embeddings.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to GPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs['sentence_embeddings']  # Extract sentence embeddings\n",
    "    return embeddings.mean(dim=1).cpu().numpy()  # Move embeddings back to CPU and convert to numpy\n",
    "\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_cosine_similarity(generated_texts, reference_texts, embedding_tokenizer, embedding_model, device, batch_size=8, max_length=512):\n",
    "    \"\"\"Computes cosine similarity, then moves embedding model off CUDA after use.\"\"\"\n",
    "\n",
    "    dataloader = DataLoader(list(zip(generated_texts, reference_texts)), batch_size=batch_size, shuffle=False)\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for batch_generated, batch_references in dataloader:\n",
    "\n",
    "        generated_embeddings = get_batch_embeddings(batch_generated, embedding_tokenizer, embedding_model, device, max_length)\n",
    "        reference_embeddings = get_batch_embeddings(batch_references, embedding_tokenizer, embedding_model, device, max_length)\n",
    "\n",
    "        batch_similarities = torch.nn.functional.cosine_similarity(\n",
    "            torch.tensor(generated_embeddings), torch.tensor(reference_embeddings)\n",
    "        ).cpu().tolist()\n",
    "\n",
    "        cosine_similarities.extend(batch_similarities)\n",
    "\n",
    "    average_similarity = sum(cosine_similarities) / len(cosine_similarities)\n",
    "    return average_similarity, cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS TO MAP RESPONSES AND SIMILARITIES TO DATASET ENTRIES ###\n",
    "\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def map_responses_and_save(test_dataset, generated_responses, output_file):\n",
    "    \"\"\"\n",
    "    Maps generated responses to the corresponding entries in the test dataset and saves to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        test_dataset (Dataset): The test dataset in Alpaca format.\n",
    "        generated_responses (list): List of generated responses from the model.\n",
    "        output_file (str): Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    if len(test_dataset) != len(generated_responses):\n",
    "        raise ValueError(\"Mismatch between dataset size and number of generated responses.\")\n",
    "\n",
    "    updated_data = []\n",
    "    for entry, response in zip(test_dataset, generated_responses):\n",
    "        entry[\"generated_response\"] = response  # Add generated response to entry\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    # Convert all float32 values to regular floats for JSON serialization\n",
    "    for entry in updated_data:\n",
    "        for key, value in entry.items():\n",
    "            if isinstance(value, float):\n",
    "                entry[key] = float(value)\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(updated_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to {output_file}\")\n",
    "\n",
    "    \n",
    "def map_similarities_and_save(input_file, cosine_similarities):\n",
    "    \"\"\"\n",
    "    Reads a JSON object from a file, adds cosine similarity scores, and writes it back to the same file.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input JSON file containing the dataset entries.\n",
    "        cosine_similarities (list): List of cosine similarity scores corresponding to the entries.\n",
    "    \"\"\"\n",
    "    # Read the existing data from the JSON file\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        updated_data = json.load(file)\n",
    "\n",
    "    # Check if the length of data matches the length of cosine similarities\n",
    "    if len(updated_data) != len(cosine_similarities):\n",
    "        raise ValueError(\"Mismatch between dataset size and number of cosine similarities.\")\n",
    "\n",
    "    # Add cosine similarity to each entry\n",
    "    for entry, similarity in zip(updated_data, cosine_similarities):\n",
    "        entry[\"cosine_similarity\"] = float(similarity)  # Add cosine similarity to entry\n",
    "\n",
    "    # Save the updated dataset back to the JSON file\n",
    "    with open(input_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(updated_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Updated dataset saved to {input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION TO EXTRACT GENERATED RESPONSES FROM JSON FILE ###\n",
    "\n",
    "import json\n",
    "\n",
    "def extract_generated_responses(json_file):\n",
    "    \"\"\"\n",
    "    Extracts the generated responses from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): Path to the JSON file containing the dataset entries.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated responses.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)  # Load the JSON data\n",
    "\n",
    "    generated_responses = []\n",
    "    reference_texts = []\n",
    "    for entry in data:\n",
    "        # Append the generated response to the list\n",
    "        generated_responses.append(entry.get(\"generated_response\"))\n",
    "        reference_texts.append(entry.get(\"output\"))\n",
    "\n",
    "\n",
    "    return generated_responses, reference_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS TO CATEGORIZE SIMILARITY BY DATASET QUESTION ###\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def categorize_and_save_entries(input_file, output_dir):\n",
    "    \"\"\"\n",
    "    Categorizes dataset entries by their prompts and saves them into separate JSON files\n",
    "    using a predefined mapping of instructions to file names.\n",
    "    \"\"\"\n",
    "\n",
    "    instruction_to_name = {\n",
    "        \"Write a header comment for this x86 assembly code snippet:\": \"header_comment\",\n",
    "        \"Comment the x86 assembly code snippet by generating a structured json object where the keys are the integer line numbers (starting at 1) and the values are the string comments. For example: {1: \\\"comment for line 1\\\", 2: \\\"comment for line 2\\\", 3: \\\"comment for line 3\\\"}\": \"inline_comments\",\n",
    "        \"Describe the intent of the given snippet of assembly code\": \"intent\",\n",
    "        \"An x86 Assembly code snippet has been partially masked. Complete the code by filling in the lines labeled '# <MASKED>'.\": \"masked\",\n",
    "        \"Answer the following question about the x86 Assembly Language:\": \"question\"\n",
    "    }\n",
    "\n",
    "    # Load the existing data from the JSON file\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        entries = json.load(file)\n",
    "\n",
    "    # Dictionary to hold lists of entries for each prompt\n",
    "    categorized_entries = defaultdict(list)\n",
    "\n",
    "    # Categorize entries based on their prompts\n",
    "    for entry in entries:\n",
    "        prompt = entry.get(\"instruction\")\n",
    "        if prompt in instruction_to_name:\n",
    "            file_key = instruction_to_name[prompt]\n",
    "            categorized_entries[file_key].append(entry)\n",
    "\n",
    "    # Save categorized entries into separate files\n",
    "    for file_key, entries in categorized_entries.items():\n",
    "        output_file = os.path.join(output_dir, f\"{file_key}.json\" )\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(entries, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Saved {len(entries)} entries to {output_file}\")\n",
    "\n",
    "\n",
    "def calculate_average_cosine_similarities(combined_average_cimilarity, output_directory, output_file):\n",
    "    \"\"\"\n",
    "    Calculates the average cosine similarity for each JSON file in the specified directory\n",
    "    and writes the results to a new JSON file.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    results[\"average\"] = combined_average_cimilarity\n",
    "\n",
    "    # Iterate through each file in the output directory\n",
    "    for filename in os.listdir(output_directory):\n",
    "        if filename.endswith(\".json\"):  # Process only JSON files\n",
    "            file_path = os.path.join(output_directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                entries = json.load(file)  # Load the entries from the file\n",
    "\n",
    "            # Extract cosine similarities and calculate the average\n",
    "            cosine_similarities = [entry.get(\"cosine_similarity\") for entry in entries if \"cosine_similarity\" in entry]\n",
    "            \n",
    "            if cosine_similarities:  # Check if there are similarities to calculate\n",
    "                average_similarity = sum(cosine_similarities) / len(cosine_similarities)\n",
    "                results[filename] = average_similarity\n",
    "\n",
    "    # Write the results to the output JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(results, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Average cosine similarities written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE RESPONSES ###\n",
    "\n",
    "generated_responses = generate_responses(model, tokenizer, test_dataset, \"cuda\", batch_size=16)\n",
    "\n",
    "output_file = f\"./summary_stats/metrics/cosine_similarities/{MODEL_LABEL}_{MODE}_test_dataset_with_responses.json\"\n",
    "map_responses_and_save(test_dataset, generated_responses, output_file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEAN UP TORCH MEMORY ###\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete the model from memory\n",
    "# del SELECTED_TOKENIZER  \n",
    "# del SELECTED_MODEL\n",
    "del model\n",
    "del tokenizer\n",
    "# del embedding_model\n",
    "# del embedding_tokenizer\n",
    "gc.collect()  # Run garbage collection\n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.ipc_collect()  \n",
    "\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e194352122144476a8b026d8cfbc52f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### LOAD EMBEDDING MODEL ###\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained(\n",
    "    EMBEDDING_MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    EMBEDDING_MODEL_PATH, \n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX 6000 Ada Generation. Max memory = 47.467 GB.\n",
      "22.748 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "### CURRENT MEMORY STATS ###\n",
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darrin_lea/miniconda3/envs/thesis/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/darrin_lea/miniconda3/envs/thesis/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4354910424990491\n",
      "[0.50244140625, 0.353515625, 0.57177734375, 0.45458984375, 0.45166015625, 0.44873046875, 0.1990966796875, 0.53173828125, 0.2435302734375, 0.416748046875, 0.2193603515625, 0.5048828125, 0.391357421875, 0.57275390625, 0.76611328125, 0.414794921875, 0.357421875, 0.435302734375, 0.4619140625, 0.1732177734375, 0.466064453125, 0.303466796875, 0.44384765625, 0.431640625, 0.358154296875, 0.53662109375, 0.422119140625, 0.373291015625, 0.3701171875, 0.2119140625, 0.130126953125, 0.1685791015625, 0.0528564453125, 0.1285400390625, 0.1353759765625, 0.0928955078125, 0.10235595703125, 0.0673828125, 0.13720703125, 0.135986328125, 0.1436767578125, 0.373779296875, 0.451416015625, 0.428955078125, 0.229736328125, 0.238037109375, 0.426513671875, 0.1553955078125, 0.474609375, 0.10888671875, 0.368408203125, 0.4091796875, 0.350341796875, 0.4169921875, 0.1575927734375, 0.460693359375, 0.30908203125, 0.319580078125, 0.423828125, 0.2298583984375, 0.374755859375, 0.1378173828125, 0.431884765625, 0.41357421875, 0.859375, 0.57958984375, 0.63134765625, 0.65380859375, 0.66162109375, 0.703125, 0.56005859375, 0.5146484375, 0.32373046875, 0.2451171875, 0.477783203125, 0.59619140625, 0.53271484375, 0.4775390625, 0.517578125, 0.25244140625, 0.61474609375, 0.6767578125, 0.71484375, 0.59521484375, 0.67578125, 0.6015625, 0.66748046875, 0.9150390625, 0.304443359375, 0.401611328125, 0.54833984375, 0.517578125, 0.47412109375, 0.54736328125, 0.336669921875, 0.451171875, 0.6181640625, 0.6552734375, 0.73486328125, 0.6845703125, 0.69482421875, 0.7900390625, 0.7060546875, 0.69580078125, 0.55517578125, 0.4013671875, 0.552734375, 0.498779296875, 0.36865234375, 0.5087890625, 0.437744140625, 0.499267578125, 0.56298828125, 0.3974609375, 0.63720703125, 0.6240234375, 0.56640625, 0.61572265625, 0.6328125, 0.623046875, 0.37890625, 0.3974609375, 0.55029296875, 0.5009765625, 0.86865234375, 0.4853515625, 0.3427734375, 0.47119140625, 0.68212890625, 0.529296875, 0.580078125, 0.425537109375, 0.4990234375, 0.410888671875, 0.407470703125, 0.716796875, 0.42333984375, 0.5078125, 0.5224609375, 0.339599609375, 0.302734375, 0.53662109375, 0.5029296875, 0.4775390625, 0.7275390625, 0.7529296875, 0.658203125, 0.7099609375, 0.70556640625, 0.6591796875, 0.397216796875, 0.8173828125, 0.60986328125, 0.4853515625, 0.56640625, 0.38427734375, 0.58837890625, 0.51171875, 0.705078125, 0.495361328125, 0.5166015625, 0.402587890625, 0.4716796875, 0.47509765625, 0.321044921875, 0.388427734375, 0.53955078125, 0.544921875, 0.521484375, 0.544921875, 0.441650390625, 0.494384765625, 0.57666015625, 0.513671875, 0.666015625, 0.42138671875, 0.576171875, 0.576171875, 0.6044921875, 0.35888671875, 0.63427734375, 0.6083984375, 0.5400390625, 0.66455078125, 0.54736328125, 0.85009765625, 0.51513671875, 0.583984375, 0.49853515625, 0.4638671875, 0.564453125, 0.5810546875, 0.630859375, 0.80859375, 0.666015625, 0.52099609375, 0.4541015625, 0.55908203125, 0.65625, 0.7978515625, 0.455322265625, 0.49658203125, 0.51025390625, 0.51806640625, 0.517578125, 0.296630859375, 0.4365234375, 0.427490234375, 0.68212890625, 0.62646484375, 0.488525390625, 0.317626953125, 0.60107421875, 0.51416015625, 0.51708984375, 0.6044921875, 0.419189453125, 0.3623046875, 0.447265625, 0.301025390625, 0.5595703125, 0.74462890625, 0.437744140625, 0.482177734375, 0.52783203125, 0.625, 0.53955078125, 0.4609375, 0.439453125, 0.57470703125, 0.468994140625, 0.45703125, 0.4462890625, 0.353515625, 0.383544921875, 0.467041015625, 0.403564453125, 0.332763671875, 0.51806640625, 0.465576171875, 0.68212890625, 0.2379150390625, 0.419189453125, 0.72021484375, 0.66455078125, 0.64208984375, 0.66552734375, 0.30712890625, 0.84228515625, 0.62451171875, 0.74072265625, 0.548828125, 0.4189453125, 0.373046875, 0.73583984375, 0.70556640625, 0.541015625, 0.4189453125, 0.4677734375, 0.4140625, 0.468505859375, 0.49560546875, 0.50634765625, 0.46630859375, 0.40771484375, 0.5751953125, 0.4033203125, 0.77099609375, 0.5673828125, 0.53759765625, 0.441650390625, 0.47998046875, 0.76806640625, 0.6552734375, 0.552734375, 0.6630859375, 0.677734375, 0.68359375, 0.529296875, 0.65478515625, 0.32421875, 0.53662109375, 0.431396484375, 0.43017578125, 0.51708984375, 0.54443359375, 0.5283203125, 0.3583984375, 0.7490234375, 0.5419921875, 0.6103515625, 0.591796875, 0.485595703125, 0.61083984375, 0.63818359375, 0.63037109375, 0.4111328125, 0.54736328125, 0.372802734375, 0.398193359375, 0.47900390625, 0.4794921875, 0.454833984375, 0.439453125, 0.384521484375, 0.387939453125, 0.51708984375, 0.459228515625, 0.43408203125, 0.402587890625, 0.5322265625, 0.56201171875, 0.51171875, 0.6650390625, 0.8720703125, 0.58642578125, 0.58837890625, 0.63720703125, 0.51025390625, 0.61572265625, 0.7236328125, 0.65771484375, 0.7099609375, 0.72119140625, 0.60009765625, 0.69873046875, 0.7197265625, 0.763671875, 0.370849609375, 0.456298828125, 0.45947265625, 0.52734375, 0.5234375, 0.52685546875, 0.5556640625, 0.446533203125, 0.501953125, 0.6171875, 0.67138671875, 0.6474609375, 0.8125, 0.70166015625, 0.7216796875, 0.69677734375, 0.53857421875, 0.47216796875, 0.57861328125, 0.58984375, 0.3564453125, 0.6572265625, 0.367431640625, 0.460693359375, 0.347412109375, 0.50390625, 0.529296875, 0.491943359375, 0.4033203125, 0.328125, 0.35302734375, 0.517578125, 0.5751953125, 0.57421875, 0.56298828125, 0.57763671875, 0.412109375, 0.62744140625, 0.6416015625, 0.498046875, 0.70166015625, 0.73876953125, 0.67236328125, 0.69677734375, 0.69482421875, 0.70263671875, 0.65185546875, 0.61376953125, 0.556640625, 0.517578125, 0.51220703125, 0.50927734375, 0.40576171875, 0.52685546875, 0.426025390625, 0.5283203125, 0.705078125, 0.59130859375, 0.72998046875, 0.310302734375, 0.61767578125, 0.6591796875, 0.6513671875, 0.60302734375, 0.45703125, 0.37548828125, 0.474609375, 0.403564453125, 0.5283203125, 0.483154296875, 0.56640625, 0.52587890625, 0.4814453125, 0.58447265625, 0.409423828125, 0.4921875, 0.5517578125, 0.433837890625, 0.7490234375, 0.6396484375, 0.6552734375, 0.53759765625, 0.6396484375, 0.50634765625, 0.78857421875, 0.3916015625, 0.64501953125, 0.76318359375, 0.56494140625, 0.479736328125, 0.619140625, 0.43896484375, 0.7890625, 0.400390625, 0.5107421875, 0.509765625, 0.495849609375, 0.468994140625, 0.456787109375, 0.6962890625, 0.515625, 0.53515625, 0.3349609375, 0.5673828125, 0.658203125, 0.517578125, 0.7060546875, 0.54150390625, 0.56103515625, 0.609375, 0.6640625, 0.63916015625, 0.345947265625, 0.5078125, 0.5478515625, 0.51904296875, 0.399169921875, 0.419921875, 0.315673828125, 0.450927734375, 0.65478515625, 0.6552734375, 0.79150390625, 0.623046875, 0.65234375, 0.68115234375, 0.68701171875, 0.5068359375, 0.43798828125, 0.515625, 0.50244140625, 0.489013671875, 0.53564453125, 0.34228515625, 0.461181640625, 0.52685546875, 0.46142578125, 0.51318359375, 0.4619140625, 0.4716796875, 0.410400390625, 0.52294921875, 0.491455078125, 0.52392578125, 0.69970703125, 0.599609375, 0.6376953125, 0.61669921875, 0.68359375, 0.6904296875, 0.85693359375, 0.6650390625, 0.2427978515625, 0.4736328125, 0.5341796875, 0.474853515625, 0.6826171875, 0.48046875, 0.41796875, 0.473876953125, 0.67919921875, 0.609375, 0.45654296875, 0.64501953125, 0.458984375, 0.50927734375, 0.65966796875, 0.70263671875, 0.595703125, 0.368408203125, 0.60693359375, 0.681640625, 0.33251953125, 0.7578125, 0.389892578125, 0.6357421875, 0.53076171875, 0.432373046875, 0.50244140625, 0.53955078125, 0.431396484375, 0.369140625, 0.4677734375, 0.54541015625, 0.3525390625, 0.2900390625, 0.52001953125, 0.434326171875, 0.444580078125, 0.1661376953125, 0.492919921875, 0.5576171875, 0.274658203125, 0.46484375, 0.3857421875, 0.48681640625, 0.5244140625, 0.51220703125, 0.513671875, 0.34326171875, 0.6748046875, 0.6787109375, 0.87841796875, 0.62939453125, 0.64013671875, 0.426513671875, 0.5546875, 0.67529296875, 0.54931640625, 0.45654296875, 0.3662109375, 0.446044921875, 0.501953125, 0.4072265625, 0.52197265625, 0.51220703125, 0.68359375, 0.60498046875, 0.728515625, 0.82568359375, 0.677734375, 0.6982421875, 0.50537109375, 0.712890625, 0.55029296875, 0.54638671875, 0.398193359375, 0.54296875, 0.5068359375, 0.5302734375, 0.431396484375, 0.44873046875, 0.482666015625, 0.457763671875, 0.40576171875, 0.494140625, 0.472412109375, 0.51025390625, 0.521484375, 0.52783203125, 0.568359375, 0.62109375, 0.5234375, 0.6025390625, 0.3359375, 0.587890625, 0.73876953125, 0.62646484375, 0.487060546875, 0.09869384765625, 0.1055908203125, 0.272216796875, 0.240966796875, 0.2496337890625, 0.10662841796875, 0.165283203125, 0.1253662109375, 0.2078857421875, 0.2275390625, 0.1241455078125, 0.48779296875, 0.34521484375, 0.1873779296875, 0.57275390625, 0.37255859375, 0.457763671875, 0.1365966796875, 0.2098388671875, 0.309326171875, 0.1956787109375, 0.225341796875, 0.57958984375, 0.434814453125, 0.132080078125, 0.42626953125, 0.5908203125, 0.12841796875, 0.1510009765625, 0.6865234375, 0.587890625, 0.6689453125, 0.38427734375, 0.60009765625, 0.415283203125, 0.428466796875, 0.12017822265625, 0.334716796875, 0.09613037109375, 0.16064453125, 0.132080078125, 0.2059326171875, 0.54345703125, 0.42138671875, 0.45166015625, 0.389404296875, 0.338623046875, 0.52880859375, 0.56884765625, 0.4482421875, 0.4013671875, 0.6171875, 0.54833984375, 0.2490234375, 0.475341796875, 0.1431884765625, 0.137451171875, 0.19287109375, 0.345703125, 0.2059326171875, 0.430908203125, 0.331298828125, 0.46240234375, 0.1402587890625, 0.2080078125, 0.29443359375, 0.367919921875, 0.36083984375, 0.323974609375, 0.2783203125, 0.314208984375, 0.50634765625, 0.29052734375, 0.285888671875, 0.50244140625, 0.23388671875, 0.35888671875, 0.2421875, 0.44482421875, 0.59326171875, 0.52685546875, 0.5107421875, 0.3134765625, 0.340576171875, 0.318603515625, 0.1146240234375, 0.60546875, 0.237060546875, 0.52197265625, 0.320556640625, 0.4287109375, 0.451416015625, 0.4541015625, 0.39794921875, 0.051239013671875, 0.420166015625, 0.2822265625, 0.6005859375, 0.326904296875, 0.431396484375, 0.131591796875, 0.431884765625, 0.36572265625, 0.2369384765625, 0.443359375, 0.42041015625, 0.203857421875, 0.2445068359375, 0.41259765625, 0.294677734375, 0.4306640625, 0.64208984375, 0.344482421875, 0.1783447265625, 0.1865234375, 0.6123046875, 0.446044921875, 0.73876953125, 0.48095703125, 0.477783203125, 0.296875, 0.50048828125, 0.2264404296875, 0.10614013671875, 0.05322265625, 0.513671875, 0.232177734375, 0.45849609375, 0.298828125, 0.08740234375, 0.462158203125, 0.1922607421875, 0.05108642578125, 0.28125, 0.348388671875, -0.021697998046875, 0.2314453125, 0.2410888671875, 0.1715087890625, 0.224609375, 0.44384765625, 0.27587890625, 0.483642578125, 0.06768798828125, 0.23193359375, 0.261474609375, 0.11993408203125, 0.351318359375, 0.5888671875, 0.75390625, 0.169921875, 0.29296875, 0.88623046875, 0.389404296875, 0.591796875, 0.06707763671875, 0.263916015625, 0.487060546875, 0.35546875, 0.55029296875, 0.50634765625, 0.247802734375, 0.495361328125, 0.6376953125, 0.392822265625, 0.865234375, 0.1707763671875, 0.36865234375, 0.81494140625, 0.437744140625, 0.9208984375, 0.283447265625, 0.335205078125, 0.51025390625, 0.497802734375, 0.411865234375, 0.430419921875, 0.386474609375, 0.40673828125, 0.297119140625, 0.373046875, 0.371337890625, 0.1533203125, 0.2685546875, 0.51220703125, 0.326904296875, 0.380859375, 0.1663818359375, 0.451171875, 0.448486328125, 0.382568359375, 0.423828125, 0.272705078125, 0.4248046875, 0.290771484375, 0.23828125, 0.32275390625, 0.300537109375, 0.26611328125, 0.235107421875, 0.1588134765625, 0.72119140625, 0.521484375, 0.2471923828125, 0.61474609375, 0.18115234375, 0.48291015625, 0.3505859375, 0.265625, 0.68994140625, 0.177490234375, 0.305419921875, 0.5654296875, 0.391845703125, 0.42724609375, 0.160888671875, 0.33154296875, -0.01114654541015625, 0.216064453125, 0.364013671875, 0.37548828125, 0.4638671875, 0.3330078125, 0.2841796875, 0.269775390625, 0.295166015625, 0.242431640625, 0.65869140625, 0.42822265625, 0.54345703125, 0.313232421875, 0.328857421875, 0.45703125, 0.313232421875, 0.410400390625, 0.1827392578125, 0.286865234375, 0.135986328125, 0.373779296875, 0.8662109375, 0.194580078125, 0.66650390625, 0.146728515625, 0.45947265625, 0.268310546875, 0.45654296875, 0.42578125, 0.209716796875, 0.357421875, 0.470458984375, 0.4619140625, 0.26171875, 0.385009765625, 0.51806640625, 0.09454345703125, 0.240478515625, 0.29345703125, 0.1090087890625, 0.151123046875, 0.3486328125, 0.301025390625, 0.2457275390625, 0.11395263671875, 0.4072265625, 0.5302734375, 0.2880859375, 0.5390625, 0.7216796875, 0.5673828125, 0.425048828125, 0.42724609375, 0.6240234375, 0.701171875, 0.263427734375, 0.1905517578125, 0.221923828125, 0.267333984375, 0.1890869140625, 0.2939453125, 0.2044677734375, 0.094482421875, 0.310791015625, 0.236328125, 0.290283203125, 0.225830078125, 0.5078125, 0.349365234375, 0.366943359375, 0.40966796875, 0.455078125, 0.2176513671875, 0.505859375, 0.5029296875, 0.308349609375, 0.220703125, 0.370361328125, 0.1287841796875, 0.6337890625, 0.4755859375, 0.363525390625, 0.37646484375, 0.283935546875, 0.37890625, 0.453125, 0.493896484375, 0.44921875, 0.445068359375, 0.260009765625, 0.41552734375, 0.7880859375, 0.412353515625, 0.49560546875, 0.254150390625, 0.86181640625, 0.5361328125, 0.179443359375, 0.386474609375, 0.333984375, 0.43505859375, 0.0579833984375, 0.431884765625, 0.23046875, 0.27490234375, 0.459716796875, 0.29833984375, 0.44970703125, 0.361572265625, 0.383056640625, 0.437744140625, 0.314697265625, 0.2073974609375, 0.74609375, 0.277587890625, 0.384033203125, 0.308837890625, 0.331298828125, 0.34228515625, 0.56494140625, 0.4326171875, 0.7919921875, 0.30859375, 0.0714111328125, 0.288818359375, 0.267578125, 0.74853515625, 0.2685546875, 0.71435546875, 0.393310546875, 0.630859375, 0.409423828125, 0.318603515625, 0.413330078125, 0.1015625, 0.47509765625, 0.55908203125, 0.6728515625, 0.1573486328125, 0.513671875, 0.544921875, 0.426025390625, 0.1602783203125, 0.162353515625, 0.24609375, 0.277099609375, 0.49658203125, 0.35986328125, 0.3837890625, 0.322509765625, 0.043701171875, 0.33349609375, 0.271728515625, 0.39453125, 0.278076171875, 0.22900390625, 0.1341552734375, 0.57373046875, 0.646484375, 0.57763671875, 0.386962890625, 0.43701171875, 0.1876220703125, 0.351318359375, 0.8330078125, 0.368896484375, 0.40380859375, 0.7353515625, 0.395751953125, 0.4033203125, 0.291259765625, 0.05145263671875, 0.38037109375, 0.2462158203125, 0.05157470703125, 0.28857421875, 0.27392578125, 0.344970703125, 0.241943359375, 0.2265625, 0.28173828125, 0.1917724609375, 0.81005859375, 0.3623046875, 0.460693359375, 0.2939453125, 0.39697265625, 0.2274169921875, 0.446044921875, 0.354248046875, 0.36376953125, 0.438232421875, 0.2218017578125, 0.101806640625, 0.211669921875, 0.4189453125, 0.136962890625, 0.59619140625, 0.48583984375, 0.461669921875, 0.46337890625, 0.59619140625, 0.5185546875, 0.0849609375, 0.19775390625, 0.330322265625, 0.50341796875, 0.1944580078125, 0.337158203125, 0.256591796875, 0.1260986328125, 0.61474609375, 0.47998046875, 0.402099609375, 0.420166015625, 0.56494140625, 0.1744384765625, 0.39404296875, 0.268310546875, 0.33642578125, 0.498046875, 0.4013671875, 0.384033203125, 0.1085205078125, 0.43212890625, 0.38330078125, 0.385986328125, 0.433349609375, 0.326171875, 0.5810546875, 0.1993408203125, 0.6708984375, 0.164306640625, 0.358154296875, 0.474853515625, 0.59765625, 0.478515625, 0.2296142578125, 0.420654296875, 0.1851806640625, 0.34619140625, 0.318359375, 0.3984375, 0.1917724609375, -0.0015878677368164062, 0.36083984375, 0.39453125, 0.459716796875, 0.0167083740234375, 0.26513671875, 0.1455078125, 0.34033203125, 0.112548828125, 0.427734375, 0.501953125, 0.290771484375, 0.302001953125, 0.359130859375, 0.322021484375, 0.1785888671875, 0.315673828125, 0.439208984375, 0.3173828125, 0.20556640625, 0.32177734375, 0.4365234375, 0.28076171875, 0.42431640625, 0.1434326171875, 0.1502685546875, 0.59033203125, 0.332275390625, 0.2763671875, 0.3330078125, 0.363525390625, 0.50439453125, 0.472900390625, 0.1763916015625, 0.279296875, 0.3310546875, 0.3349609375, 0.386962890625, 0.417236328125, 0.27734375, 0.32470703125, 0.52783203125, 0.69189453125, 0.498046875, 0.352783203125, 0.412353515625, -0.0025119781494140625, 0.6240234375, 0.122314453125, 0.1751708984375, 0.431884765625, 0.69287109375, 0.312744140625, 0.4150390625, 0.353515625, 0.435302734375, 0.2442626953125, 0.1490478515625, 0.4423828125, 0.193603515625, 0.08392333984375, 0.334228515625, 0.220703125, -0.0041046142578125, 0.4765625, 0.5595703125, 0.28125, 0.544921875, 0.48291015625, 0.380859375, 0.299560546875, 0.2266845703125, 0.50634765625, 0.1546630859375, 0.352783203125, 0.4658203125, 0.1324462890625, 0.293701171875, 0.417236328125, 0.489501953125, 0.2467041015625, 0.2119140625, 0.443115234375, 0.287841796875, 0.360595703125, 0.43896484375, 0.128662109375, 0.60986328125, 0.74853515625, 0.34228515625, 0.2249755859375, 0.208251953125, 0.39013671875, 0.035888671875, 0.284423828125, 0.40234375, 0.345703125, 0.44140625, 0.350830078125, 0.381591796875, 0.1041259765625, 0.494873046875, 0.32666015625, 0.384521484375, 0.2333984375, 0.4697265625, 0.6142578125, 0.375244140625, 0.295166015625, 0.52197265625, 0.2044677734375, 0.2529296875, 0.537109375, 0.1717529296875, 0.46435546875, 0.30517578125, 0.3173828125, 0.20263671875, 0.2060546875, 0.29541015625, 0.31787109375, 0.2108154296875, 0.40771484375, 0.2342529296875, 0.481689453125, 0.472900390625, 0.2919921875, 0.432861328125, 0.7236328125, 0.42919921875, 0.1951904296875, 0.2919921875, 0.30224609375, 0.4560546875, 0.329345703125, 0.134521484375, 0.35400390625, 0.392578125, 0.6826171875, 0.311767578125, 0.36962890625, 0.047637939453125, 0.422119140625, 0.4130859375, 0.03656005859375, 0.1649169921875, 0.59423828125, 0.439208984375, 0.496826171875, 0.5078125, 0.8173828125, 0.274658203125, 0.5302734375, 0.36376953125, 0.314453125, 0.44921875, 0.40966796875, 0.365234375, 0.240478515625, 0.43212890625]\n",
      "Updated dataset saved to ./summary_stats/metrics/cosine_similarities/codellama-13B_BASE_test_dataset_with_responses.json\n",
      "Saved 26 entries to ./summary_stats/metrics/cosine_similarities/codellama-13B-BASE/header_comment.json\n",
      "Saved 39 entries to ./summary_stats/metrics/cosine_similarities/codellama-13B-BASE/inline_comments.json\n",
      "Saved 512 entries to ./summary_stats/metrics/cosine_similarities/codellama-13B-BASE/intent.json\n",
      "Saved 45 entries to ./summary_stats/metrics/cosine_similarities/codellama-13B-BASE/masked.json\n",
      "Saved 577 entries to ./summary_stats/metrics/cosine_similarities/codellama-13B-BASE/question.json\n",
      "Average cosine similarities written to ./summary_stats/metrics/codellama-13B_BASE_similarity_metrics.json\n"
     ]
    }
   ],
   "source": [
    "### CALCULATE SIMILARITY OVER WHOLE DATASET ###\n",
    "\n",
    "reference_texts = [sample['output'] for sample in test_dataset]\n",
    "\n",
    "# need to format the generated_responses\n",
    "\n",
    "average_similarity, cosine_similarities = compute_cosine_similarity(\n",
    "    generated_responses, reference_texts, embedding_tokenizer, embedding_model, \"cuda\"\n",
    ")\n",
    "\n",
    "print(average_similarity)\n",
    "print(cosine_similarities)\n",
    "\n",
    "input_file = f\"./summary_stats/metrics/cosine_similarities/{MODEL_LABEL}_{MODE}_test_dataset_with_responses.json\"\n",
    "output_dir = f\"./summary_stats/metrics/cosine_similarities/{MODEL_LABEL}-{MODE}\"\n",
    "\n",
    "map_similarities_and_save(input_file, cosine_similarities)\n",
    "\n",
    "categorize_and_save_entries(input_file, output_dir)\n",
    "\n",
    "calculate_average_cosine_similarities(average_similarity, output_dir, f\"./summary_stats/metrics/{MODEL_LABEL}_{MODE}_similarity_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Write a header comment for this x86 assembly code snippet:\n",
      "\n",
      "### Input:\n",
      "\n",
      "        section .data\n",
      "            message db \"Hello, World!\", 0xA\n",
      "            msg_len equ $ - message\n",
      "\n",
      "        section .text\n",
      "            global _start\n",
      "\n",
      "        _start:\n",
      "            mov rax, 1\n",
      "            mov rdi, 1\n",
      "            mov rsi, message\n",
      "            mov rdx, msg_len\n",
      "            syscall\n",
      "\n",
      "            mov rax, 60\n",
      "            xor rdi, rdi\n",
      "            syscall\n",
      "\n",
      "      \n",
      "\n",
      "### Response:\n",
      "; This snippet prints \"Hello, World!\" to the console and then exits.\n",
      "; It uses the write syscall to print the message and the exit syscall\n",
      "; to terminate the program.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TEST WITH A SANDBOX EXAMPLE ###\n",
    "\n",
    "MODEL = model\n",
    "TOKENIZER = tokenizer\n",
    "\n",
    "\n",
    "prompt = '''Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Write a header comment for this x86 assembly code snippet:\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}'''\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "      '''\n",
    "        section .data\n",
    "            message db \"Hello, World!\", 0xA\n",
    "            msg_len equ $ - message\n",
    "\n",
    "        section .text\n",
    "            global _start\n",
    "\n",
    "        _start:\n",
    "            mov rax, 1\n",
    "            mov rdi, 1\n",
    "            mov rsi, message\n",
    "            mov rdx, msg_len\n",
    "            syscall\n",
    "\n",
    "            mov rax, 60\n",
    "            xor rdi, rdi\n",
    "            syscall\n",
    "\n",
    "      ''',\n",
    "      \"\"\n",
    "\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "outputs = MODEL.generate(**inputs, max_new_tokens = 300, use_cache = True)\n",
    "output_text = TOKENIZER.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
